{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee67aa71",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black\">\n",
    "<b><center><font size=\"4\">Ciência de Dados em Larga Escala</font></center></b>\n",
    "\n",
    "<b><center><font size=\"3\">Project 2</font></center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698cbe58",
   "metadata": {},
   "source": [
    "**Notebook Developed by**: Rafael Antunes Lourenço<br>\n",
    "**Número:** 48115 <br>\n",
    "**Email:**  rafael.a.lourenco@ubi.pt<br>\n",
    "<hr>\n",
    "\n",
    "<p><a href=\"project1.ipynb\" title=\"Download Notebook\" download><img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/download.jpg\" align = \"left\" width=\"50\" height=\"50\" alt=\"Download Notebook\"></a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff01a35-ef15-43fd-b66e-44458c637199",
   "metadata": {},
   "source": [
    "# Introdução "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3177714f-076f-4c5a-bb09-12e684f7742d",
   "metadata": {},
   "source": [
    "Este projeto aborda a análise de um extenso conjunto de dados meteorológicos diários, utilizando técnicas de chunking para processamento eficiente. Inicialmente, exploramos as variáveis disponíveis, identificando melhorias nos tipos de dados e tratando valores nulos. Em seguida, realizamos análises estatísticas, como a determinação dos anos mais antigos e recentes para cada estação, e o cálculo da temperatura média diária. Além disso, focamos nas estações em Portugal, selecionando e filtrando os dados relevantes, e substituímos os identificadores das estações pelos seus nomes correspondentes para uma melhor compreensão. Essa abordagem permite extrair insights valiosos sobre as condições climáticas ao longo do tempo e em diferentes regiões, mesmo lidando com conjuntos de dados extensos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d8f8f-cd38-4b48-90c5-d074f2b8f6fb",
   "metadata": {},
   "source": [
    "# Suprimir Warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387f1ac-551d-420b-9b8d-bcff1a2d4433",
   "metadata": {},
   "source": [
    "Este pequeno excerto de código apenas serve para evitar o aparecimento das mensagens de aviso no decorrer do código, usei isto apenas porque antes de alterarmos as variáveis, havia diferentes tipos de variável em linhas iguais, o que causava muitos avisos e tornava o output cheio de avisos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f51dd5-3236-4ace-a734-0d898f9e98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "# Suprimir todos os warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05ca61-5325-4ad5-b39f-4c0f3bdffab2",
   "metadata": {},
   "source": [
    "# Carregar o dataset, dividir o mesmo em chunks e guardar cada chunk no diretório /chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f1afe-008b-4503-9726-ed33225cedc8",
   "metadata": {},
   "source": [
    "Este código lê um arquivo CSV de dados meteorológicos diários em chunks de tamanho definido (no caso, 50000) e salva cada chunk como um novo arquivo CSV. O objetivo é lidar com conjuntos de dados extensos de forma mais eficiente, dividindo-os em partes menores que podem ser processadas separadamente.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0675f559-3924-46ef-b881-44afb8127ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=50000\n",
    "batch_no=1\n",
    "\n",
    "for chunk_df in pd.read_csv('global_climate_data/ghcnd_daily/ghcnd_daily.csv',chunksize=chunk_size):\n",
    "    chunk_df.to_csv('global_climate_data/ghcnd_daily/chunks/chunks'+str(batch_no)+'.csv',index=False)\n",
    "    print(batch_no)\n",
    "    batch_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a5594-1e38-4b17-bd51-6250723ab03c",
   "metadata": {},
   "source": [
    "# Exercicío 1 - Selecionei o chunk 200 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f76d06-e5ef-42cc-b428-62d915878815",
   "metadata": {},
   "source": [
    "Escolhi o chunk 200 aleatoriamente, não existiu nenhum critério para a sua escolha. \n",
    "Defini a variável *path* com o caminho para o chunk que escolhi, para a variável *selected_columns* usei um ciclo for para selecionar todas as colunas 'id', 'year', 'month' e 'element' de todos os dias do mes, desde o value 1 ao 31.\n",
    "De seguida, apenas usei a função pd.read_csv() para ler as colunas desejadas no chunk escolhido e atribui essess valores à variável *chunk200*.\n",
    "Após isso, usei a função replace() para substituir todos os valores '-9999' por 'NA', usando o pd.NA.\n",
    "Por final usei apenas o dtype para me indicar os tipos de variável que eu selecionei anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c178b4-1a83-4a42-b3e4-e1d1f6551423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         object\n",
      "year        int64\n",
      "month       int64\n",
      "element    object\n",
      "value1     object\n",
      "value2     object\n",
      "value3     object\n",
      "value4     object\n",
      "value5     object\n",
      "value6     object\n",
      "value7     object\n",
      "value8     object\n",
      "value9     object\n",
      "value10    object\n",
      "value11    object\n",
      "value12    object\n",
      "value13    object\n",
      "value14    object\n",
      "value15    object\n",
      "value16    object\n",
      "value17    object\n",
      "value18    object\n",
      "value19    object\n",
      "value20    object\n",
      "value21    object\n",
      "value22    object\n",
      "value23    object\n",
      "value24    object\n",
      "value25    object\n",
      "value26    object\n",
      "value27    object\n",
      "value28    object\n",
      "value29    object\n",
      "value30    object\n",
      "value31    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Caminho para o arquivo do chunk 200\n",
    "path = 'global_climate_data/ghcnd_daily/chunks/chunks200.csv'\n",
    "\n",
    "# Selecionar todas as colunas de 'value1' até 'value31'\n",
    "selected_columns = ['id', 'year', 'month', 'element'] + [f'value{i}' for i in range(1, 32)]\n",
    "\n",
    "# Leitura do chunk 200\n",
    "chunk200 = pd.read_csv(path, usecols=selected_columns)\n",
    "\n",
    "# Substituir -9999 por NaN\n",
    "chunk200.replace(-9999, pd.NA, inplace=True)\n",
    "\n",
    "# Verificar os tipos de dados\n",
    "print(chunk200.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884440d-9533-4143-9eab-33b7c0496d5c",
   "metadata": {},
   "source": [
    "## Exercicío 1 - Converter as variáveis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c6fb1-0412-4207-98c3-3d2e143186c5",
   "metadata": {},
   "source": [
    "Este código converte as colunas 'value1' a 'value31' para float, 'element' para categoria, 'year' para int16 e 'month' para int8, com objetivo de otimizar o uso de memória e facilitar a manipulação dos dados.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a766de71-ec9d-4d5d-844b-bcaf4b8bd742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           object\n",
      "year          int16\n",
      "month          int8\n",
      "element    category\n",
      "value1      float64\n",
      "value2      float64\n",
      "value3      float64\n",
      "value4      float64\n",
      "value5      float64\n",
      "value6      float64\n",
      "value7      float64\n",
      "value8      float64\n",
      "value9      float64\n",
      "value10     float64\n",
      "value11     float64\n",
      "value12     float64\n",
      "value13     float64\n",
      "value14     float64\n",
      "value15     float64\n",
      "value16     float64\n",
      "value17     float64\n",
      "value18     float64\n",
      "value19     float64\n",
      "value20     float64\n",
      "value21     float64\n",
      "value22     float64\n",
      "value23     float64\n",
      "value24     float64\n",
      "value25     float64\n",
      "value26     float64\n",
      "value27     float64\n",
      "value28     float64\n",
      "value29     float64\n",
      "value30     float64\n",
      "value31     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Converter as colunas 'value1' e 'value31' para float\n",
    "for col in [f'value{i}' for i in range(1, 32)]:\n",
    "    chunk200[col] = pd.to_numeric(chunk200[col])\n",
    "\n",
    "# Converter a coluna 'element' para categoria\n",
    "chunk200['element'] = chunk200['element'].astype('category')\n",
    "\n",
    "# Converter a coluna 'year' para int16\n",
    "chunk200['year'] = chunk200['year'].astype('int16')\n",
    "\n",
    "# Converter a coluna 'month' para int8\n",
    "chunk200['month'] = chunk200['month'].astype('int8')\n",
    "\n",
    "# Verificar os tipos de dados atualizados\n",
    "print(chunk200.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8168615-1b07-48b8-8274-22f57466a1f7",
   "metadata": {},
   "source": [
    "# Exercicío 2 - Calcular a percentagem de valores nulos (NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68a14e-c1c2-4227-a30a-2a67217ae146",
   "metadata": {},
   "source": [
    "Neste bloco de código, eu itero  sobre as colunas selecionadas do DataFrame 'chunk200'. Para cada coluna, o método 'isnull()' é aplicado, devolvendo uma série booleana indicando se cada valor é nulo ou não. Em seguida, o método 'sum()' é utilizado para somar os valores True, ou seja, contabilizo o número total de valores nulos naquela coluna. Esta contagem é então usada para calcular a percentagem de valores nulos em relação ao total de observações no DataFrame. Essa percentagem é armazenada em um dicionário, onde a chave é o nome da coluna e o valor é a percentagem de valores nulos.\n",
    " Finalmente, o loop 'for' é utilizado para imprimir cada coluna juntamente com sua respectiva percentagem de valores nulos.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8ac46c-ccf2-44dc-9c63-2cc503477841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0.00%\n",
      "year: 0.00%\n",
      "month: 0.00%\n",
      "element: 0.00%\n",
      "value1: 2.70%\n",
      "value2: 2.47%\n",
      "value3: 2.39%\n",
      "value4: 2.37%\n",
      "value5: 2.32%\n",
      "value6: 2.23%\n",
      "value7: 2.20%\n",
      "value8: 2.09%\n",
      "value9: 2.04%\n",
      "value10: 2.13%\n",
      "value11: 2.24%\n",
      "value12: 2.16%\n",
      "value13: 2.12%\n",
      "value14: 2.12%\n",
      "value15: 2.18%\n",
      "value16: 2.23%\n",
      "value17: 2.17%\n",
      "value18: 2.20%\n",
      "value19: 2.20%\n",
      "value20: 2.21%\n",
      "value21: 2.23%\n",
      "value22: 2.22%\n",
      "value23: 2.21%\n",
      "value24: 2.35%\n",
      "value25: 2.64%\n",
      "value26: 2.43%\n",
      "value27: 2.35%\n",
      "value28: 2.45%\n",
      "value29: 8.58%\n",
      "value30: 10.52%\n",
      "value31: 43.46%\n"
     ]
    }
   ],
   "source": [
    "null_percentage = {}\n",
    "\n",
    "# Calcular a percentagem de valores nulos para cada variável\n",
    "for col in selected_columns:\n",
    "    # Contar o número de valores nulos na coluna\n",
    "    null_count = chunk200[col].isnull().sum()\n",
    "    \n",
    "    # Calcular a percentagem de valores nulos\n",
    "    percentage = (null_count / len(chunk200)) * 100\n",
    "    \n",
    "    # Armazenar a percentagem de valores nulos para a variável atual\n",
    "    null_percentage[col] = percentage\n",
    "\n",
    "for col, percentage in null_percentage.items():\n",
    "    print(f'{col}: {percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841584b-8865-463e-91fe-44341561047b",
   "metadata": {},
   "source": [
    "# Exercicío 3 -  Determinar, para cada estação, o ano mais antigo e o mais recente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56af39-b519-4392-9244-c9c7595fcffe",
   "metadata": {},
   "source": [
    "\r\n",
    "Estblocoho de código realiza as seguintes operações:\r\n",
    "\r\n",
    "1. **Leitura dos Arquivos**: \r\n",
    "   - Lê o arquivo 'ghcnd-stations.txt' para obter informações sobre as estações meteorológicas.\r\n",
    "   - Lê o arquivo 'chunks200.csv' que contém dados meteorológicos diários.\r\n",
    "\r\n",
    "2. **Merge de Dados**:\r\n",
    "   - Combina os dados meteorológicos do arquivo 'chunks200.csv' com as informações das estações do arquivo 'ghcnd-stations.txt' com base no identificador da estação ('id'), utilizando a função `pd.merge()`. Os dados são combinados usando a coluna 'id' como chave primária e os dados meteorológicos são adicionados às informações da estação.\r\n",
    "\r\n",
    "3. **Conversão de Tipos de Dados**:\r\n",
    "   - Converte a coluna 'year' para o tipo de dados 'int16' para otimização de memória.\r\n",
    "\r\n",
    "4. **Agrupamento de Dados**:\r\n",
    "   - Agrupa os dados combinados por estação usando a função `groupby()`.\r\n",
    "\r\n",
    "5. **Determinação do Ano Mais Antigo e Mais Recente**:\r\n",
    "   - Determina o ano mais antigo e o mais recente para cada estação, encontrando o mínimo e o máximo da coluna 'year' em cada grupo de estação.\r\n",
    "\r\n",
    "6. **Impressão dos Resultados**:\r\n",
    "   - Imprime o resultado para cada estação, mostrando o ano mais antigo temporais e espaciais.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a570ada-6ba8-4376-8845-7005046de7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estação USC00235861: Ano mais antigo: 1938, Ano mais recente: 1968\n",
      "Estação USC00235862: Ano mais antigo: 1960, Ano mais recente: 2013\n",
      "Estação USC00235916: Ano mais antigo: 1985, Ano mais recente: 2019\n",
      "Estação USC00235950: Ano mais antigo: 2007, Ano mais recente: 2010\n",
      "Estação USC00235976: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00235987: Ano mais antigo: 1898, Ano mais recente: 2019\n",
      "Estação USC00235999: Ano mais antigo: 1893, Ano mais recente: 1894\n",
      "Estação USC00236009: Ano mais antigo: 1980, Ano mais recente: 1989\n",
      "Estação USC00236012: Ano mais antigo: 1956, Ano mais recente: 2019\n",
      "Estação USC00236031: Ano mais antigo: 1898, Ano mais recente: 1906\n",
      "Estação USC00236040: Ano mais antigo: 1893, Ano mais recente: 1901\n",
      "Estação USC00236045: Ano mais antigo: 1963, Ano mais recente: 2012\n",
      "Estação USC00236052: Ano mais antigo: 1898, Ano mais recente: 1910\n",
      "Estação USC00236226: Ano mais antigo: 1894, Ano mais recente: 1894\n",
      "Estação USC00236233: Ano mais antigo: 1893, Ano mais recente: 1920\n",
      "Estação USC00236294: Ano mais antigo: 1893, Ano mais recente: 1912\n",
      "Estação USC00236302: Ano mais antigo: 1963, Ano mais recente: 2004\n",
      "Estação USC00236357: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00236402: Ano mais antigo: 1955, Ano mais recente: 2019\n",
      "Estação USC00236452: Ano mais antigo: 2003, Ano mais recente: 2019\n",
      "Estação USC00236460: Ano mais antigo: 1946, Ano mais recente: 2019\n",
      "Estação USC00236493: Ano mais antigo: 1897, Ano mais recente: 1933\n",
      "Estação USC00236509: Ano mais antigo: 1893, Ano mais recente: 1894\n",
      "Estação USC00236524: Ano mais antigo: 1908, Ano mais recente: 1909\n",
      "Estação USC00236633: Ano mais antigo: 1950, Ano mais recente: 1950\n",
      "Estação USC00236641: Ano mais antigo: 1907, Ano mais recente: 2019\n",
      "Estação USC00236683: Ano mais antigo: 1956, Ano mais recente: 1959\n",
      "Estação USC00236739: Ano mais antigo: 1966, Ano mais recente: 2009\n",
      "Estação USC00236745: Ano mais antigo: 1995, Ano mais recente: 2019\n",
      "Estação USC00236775: Ano mais antigo: 1946, Ano mais recente: 2013\n",
      "Estação USC00236777: Ano mais antigo: 1961, Ano mais recente: 2019\n",
      "Estação USC00236791: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00236799: Ano mais antigo: 1952, Ano mais recente: 1965\n",
      "Estação USC00236804: Ano mais antigo: 1965, Ano mais recente: 2012\n",
      "Estação USC00236826: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00236866: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00236872: Ano mais antigo: 1903, Ano mais recente: 1906\n",
      "Estação USC00236874: Ano mais antigo: 2011, Ano mais recente: 2016\n",
      "Estação USC00237051: Ano mais antigo: 2011, Ano mais recente: 2018\n",
      "Estação USC00237102: Ano mais antigo: 1893, Ano mais recente: 1900\n",
      "Estação USC00237112: Ano mais antigo: 1896, Ano mais recente: 1902\n",
      "Estação USC00237116: Ano mais antigo: 2006, Ano mais recente: 2019\n",
      "Estação USC00237122: Ano mais antigo: 1962, Ano mais recente: 1981\n",
      "Estação USC00237223: Ano mais antigo: 1907, Ano mais recente: 1907\n",
      "Estação USC00237263: Ano mais antigo: 1897, Ano mais recente: 2019\n",
      "Estação USC00237300: Ano mais antigo: 1980, Ano mais recente: 2019\n",
      "Estação USC00237309: Ano mais antigo: 1976, Ano mais recente: 2019\n",
      "Estação USC00237388: Ano mais antigo: 1895, Ano mais recente: 1926\n",
      "Estação USC00237397: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00237398: Ano mais antigo: 1975, Ano mais recente: 2019\n",
      "Estação USC00237445: Ano mais antigo: 1908, Ano mais recente: 1965\n",
      "Estação USC00237452: Ano mais antigo: 1968, Ano mais recente: 2018\n",
      "Estação USC00237462: Ano mais antigo: 1976, Ano mais recente: 1980\n",
      "Estação USC00237465: Ano mais antigo: 1911, Ano mais recente: 1973\n",
      "Estação USC00237470: Ano mais antigo: 1938, Ano mais recente: 1957\n",
      "Estação USC00237475: Ano mais antigo: 1980, Ano mais recente: 2016\n",
      "Estação USC00237506: Ano mais antigo: 1903, Ano mais recente: 2019\n",
      "Estação USC00237514: Ano mais antigo: 1946, Ano mais recente: 2019\n",
      "Estação USC00237521: Ano mais antigo: 1904, Ano mais recente: 1904\n",
      "Estação USC00237578: Ano mais antigo: 1948, Ano mais recente: 2019\n",
      "Estação USC00237632: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00237645: Ano mais antigo: 1921, Ano mais recente: 2009\n",
      "Estação USC00237656: Ano mais antigo: 2003, Ano mais recente: 2011\n",
      "Estação USC00237674: Ano mais antigo: 1896, Ano mais recente: 1960\n",
      "Estação USC00237720: Ano mais antigo: 1928, Ano mais recente: 2019\n",
      "Estação USC00237770: Ano mais antigo: 1894, Ano mais recente: 1959\n",
      "Estação USC00237772: Ano mais antigo: 1950, Ano mais recente: 2019\n",
      "Estação USC00237862: Ano mais antigo: 1985, Ano mais recente: 2019\n",
      "Estação USC00237963: Ano mais antigo: 1957, Ano mais recente: 2019\n",
      "Estação USC00238003: Ano mais antigo: 1893, Ano mais recente: 2011\n",
      "Estação USC00238043: Ano mais antigo: 1893, Ano mais recente: 2005\n",
      "Estação USC00238051: Ano mais antigo: 1897, Ano mais recente: 2019\n",
      "Estação USC00238082: Ano mais antigo: 1970, Ano mais recente: 2019\n",
      "Estação USC00238171: Ano mais antigo: 1977, Ano mais recente: 1977\n",
      "Estação USC00238184: Ano mais antigo: 1963, Ano mais recente: 2019\n",
      "Estação USC00238223: Ano mais antigo: 1941, Ano mais recente: 2019\n",
      "Estação USC00238292: Ano mais antigo: 2008, Ano mais recente: 2019\n",
      "Estação USC00238444: Ano mais antigo: 1895, Ano mais recente: 2019\n",
      "Estação USC00238456: Ano mais antigo: 2011, Ano mais recente: 2014\n",
      "Estação USC00238466: Ano mais antigo: 1980, Ano mais recente: 2019\n",
      "Estação USC00238515: Ano mais antigo: 1937, Ano mais recente: 2009\n",
      "Estação USC00238523: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00238525: Ano mais antigo: 1954, Ano mais recente: 1955\n",
      "Estação USC00238526: Ano mais antigo: 2007, Ano mais recente: 2007\n",
      "Estação USC00238542: Ano mais antigo: 1989, Ano mais recente: 1990\n",
      "Estação USC00238569: Ano mais antigo: 2003, Ano mais recente: 2006\n",
      "Estação USC00238571: Ano mais antigo: 1963, Ano mais recente: 1995\n",
      "Estação USC00238577: Ano mais antigo: 1911, Ano mais recente: 2019\n",
      "Estação USC00238581: Ano mais antigo: 1923, Ano mais recente: 1923\n",
      "Estação USC00238583: Ano mais antigo: 2004, Ano mais recente: 2014\n",
      "Estação USC00238603: Ano mais antigo: 1904, Ano mais recente: 2019\n",
      "Estação USC00238610: Ano mais antigo: 1897, Ano mais recente: 1905\n",
      "Estação USC00238620: Ano mais antigo: 1962, Ano mais recente: 2012\n",
      "Estação USC00238664: Ano mais antigo: 2002, Ano mais recente: 2012\n",
      "Estação USC00238700: Ano mais antigo: 1939, Ano mais recente: 2019\n",
      "Estação USC00238703: Ano mais antigo: 1953, Ano mais recente: 1956\n",
      "Estação USC00238712: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00238725: Ano mais antigo: 1893, Ano mais recente: 2019\n",
      "Estação USC00238733: Ano mais antigo: 1893, Ano mais recente: 1984\n",
      "Estação USC00238738: Ano mais antigo: 1918, Ano mais recente: 1918\n",
      "Estação USC00238740: Ano mais antigo: 2009, Ano mais recente: 2019\n",
      "Estação USC00238746: Ano mais antigo: 2009, Ano mais recente: 2019\n",
      "Estação USC00238754: Ano mais antigo: 1941, Ano mais recente: 2019\n",
      "Estação USC00238770: Ano mais antigo: 2011, Ano mais recente: 2019\n",
      "Estação USC00238777: Ano mais antigo: 1941, Ano mais recente: 2019\n",
      "Estação USC00238791: Ano mais antigo: 1946, Ano mais recente: 1964\n"
     ]
    }
   ],
   "source": [
    "# Ler o arquivo ghcnd-stations.txt para obter informações sobre as estações\n",
    "stations_info = pd.read_csv('global_climate_data/ghcnd-stations.txt', header=None, names=['id', 'name', 'latitude', 'longitude', 'elevation', 'state', 'country', 'unknown'])\n",
    "\n",
    "# Ler o arquivo ghcnd_daily.csv\n",
    "chunk200 = pd.read_csv('global_climate_data/ghcnd_daily/chunks/chunks200.csv')\n",
    "\n",
    "# Merge entre as informações da estação e os dados meteorológicos\n",
    "merged_data = pd.merge(chunk200, stations_info, on='id', how='left')\n",
    "\n",
    "# Converter a coluna 'year' para int16\n",
    "merged_data['year'] = merged_data['year'].astype('int16')\n",
    "\n",
    "# Agrupar os dados por estação\n",
    "station_groups = merged_data.groupby('id')\n",
    "\n",
    "# Determinar o ano mais antigo e o mais recente para cada estação\n",
    "oldest_year = station_groups['year'].min()\n",
    "latest_year = station_groups['year'].max()\n",
    "\n",
    "# Mostrar os resultados\n",
    "for station_id, (oldest, latest) in zip(oldest_year.index, zip(oldest_year, latest_year)):\n",
    "    print(f\"Estação {station_id}: Ano mais antigo: {oldest}, Ano mais recente: {latest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42de67-7874-4e23-bbbc-f635a6c2bdef",
   "metadata": {},
   "source": [
    "# Exercicío 4 - Determinar a temperatura média para cada observação (estação/ano/mês)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574affe-dc7c-498d-9f21-f2180d34de86",
   "metadata": {},
   "source": [
    "1. **Seleção das Colunas**:\n",
    "   - Seleciona apenas as colunas do DataFrame que começam com o prefixo 'value' usando o método `filter()` com o argumento `like='value'`.\n",
    "\n",
    "2. **Cálculo da Temperatura Média**:\n",
    "   - Calcula a temperatura média para cada observação (estação/ano/mês) através da média das colunas selecionadas anteriormente. A média é calculada ao longo do eixo das colunas (axis=1) usando o método `mean()`.\n",
    "\n",
    "3. **Criação de uma Nova Coluna**:\n",
    "   - Adiciona uma nova coluna chamada 'daily_avg_temp' ao DataFrame 'chunk200', que armazena a temperatura média calculada para cada observação.\n",
    "\n",
    "4. **Impressão das Primeiras Linhas**:\n",
    "   - Imprime as primeiras linhas do DataFrame atualizado, mostrando a nova coluna 'daily_avg_temp' juntamente com as outras colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9205d3aa-8076-454e-abe7-3309fcaa981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  year  month element value1 value2 value3 value4 value5 value6  \\\n",
      "0  USC00235861  1938      5    TMAX    294    267    272    244    217    217   \n",
      "1  USC00235861  1938      6    TMAX    267    300    328    306    294    294   \n",
      "2  USC00235861  1938      7    TMAX    339    350    367    367    367    367   \n",
      "3  USC00235861  1938      8    TMAX    339    356    350    356    361    372   \n",
      "4  USC00235861  1938      9    TMAX    367    322    317    328    350    339   \n",
      "\n",
      "   ... value23 value24 value25 value26 value27 value28 value29 value30  \\\n",
      "0  ...     200     222     244     250     267     289     289     300   \n",
      "1  ...     322     333     322     239     217     267     278     311   \n",
      "2  ...     344     350     356     350     339     267     294     328   \n",
      "3  ...     389     389     394     406     383     394     389     294   \n",
      "4  ...     333     339     322     361     322     311     306     350   \n",
      "\n",
      "  value31 daily_avg_temp  \n",
      "0     294     245.967742  \n",
      "1    <NA>          283.5  \n",
      "2     333     335.354839  \n",
      "3     378     364.580645  \n",
      "4    <NA>          302.8  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Selecionar apenas as colunas que começam com 'value'\n",
    "value_columns = chunk200.filter(like='value')\n",
    "\n",
    "# Calcular a temperatura média para cada observação (estação/ano/mês)\n",
    "chunk200['daily_avg_temp'] = value_columns.mean(axis=1)\n",
    "\n",
    "# Mostrar as primeiras linhas do DataFrame com a nova coluna 'daily_avg_temp'\n",
    "print(chunk200.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1138e-c92f-4cf2-b8f0-b195ea0dfe8c",
   "metadata": {},
   "source": [
    "# Exercicío 5 - Agrupar os dados por nome de estação e ano aplicando a função mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972c75d-742b-4dac-a204-3ee9b777435b",
   "metadata": {},
   "source": [
    "1. **Agrupamento de Dados**:\r\n",
    "   - Os dados do DataFrame 'chunk200' são agrupados pelo nome da estação ('id') e pelo ano ('year') utilizando o método `groupby()`.\r\n",
    "\r\n",
    "2. **Aplicação da Função Mean**:\r\n",
    "   - Para cada grupo formado pela combinação única de nome da estação e ano, a função `mean()` é aplicada à coluna 'daily_avg_temp' para calcular a temperatura média.\r\n",
    "\r\n",
    "3. **Resultados Impressos**:\r\n",
    "   - Os resultados do cálculo da temperatura média são impressos, mostrando apenas a coluna 'daily_avg_temp' para cada combinação de estaçãoe ano.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d980986a-2a89-43d7-8ddd-5c2d16310e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           year\n",
      "USC00235861  1938    259.607124\n",
      "             1939     218.81918\n",
      "             1940    201.160005\n",
      "             1941    219.060762\n",
      "             1942    206.324747\n",
      "                        ...    \n",
      "USC00238791  1960    190.600002\n",
      "             1961    191.209082\n",
      "             1962    194.580421\n",
      "             1963    201.546845\n",
      "             1964    203.932316\n",
      "Name: daily_avg_temp, Length: 4408, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Agrupar os dados por nome da estação e ano, aplicando a função mean\n",
    "station_year_avg = chunk200.groupby(['id', 'year'])['daily_avg_temp'].mean()\n",
    "\n",
    "# Mostrar apenas os resultados para a coluna 'daily_avg_temp'\n",
    "print(station_year_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2d575-92c1-4b5c-94c8-fcb78eb409b3",
   "metadata": {},
   "source": [
    "# Exercicío 6 - Selecionar os dados apenas das estações portuguesast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e89227-717c-4b65-9a9b-11fc03eee3f7",
   "metadata": {},
   "source": [
    "1. **Leitura do Arquivo de Informações das Estações**:\r\n",
    "   - O arquivo 'ghcnd-stations.txt' é lido usando o método `pd.read_fwf()` para carregar os dados em um DataFrame. Esse método é utilizado devido ao formato de largura fixa do arquivo.\r\n",
    "   - As larguras das colunas são especificadas no parâmetro 'widths'.\r\n",
    "   - Como o arquivo não possui cabeçalho, o parâmetro 'header' é definido como None.\r\n",
    "\r\n",
    "2. **Renomeação das Colunas**:\r\n",
    "   - As colunas do DataFrame resultante são renomeadas para uma melhor identificação dos dados.\r\n",
    "\r\n",
    "3. **Filtragem das Estações Meteorológicas Portuguesas**:\r\n",
    "   - As estações meteorológicas portuguesas são filtradas com base no código do país ('id') começando com 'PO'.\r\n",
    "\r\n",
    "4. **Seleção das Cinco Estações Desejadas**:\r\n",
    "   - As cinco estações desejadas ('Horta', 'Funchal', 'Lisboa', 'Castelo Branco' e 'Faro') são selecionadas com base nos nomes das estações.\r\n",
    "\r\n",
    "5. **Leitura do Arquivo de Dados Diários**:\r\n",
    "   - O arquivo 'chunks200.csv' é lido para obter os dados meteorológicos diários.\r\n",
    "\r\n",
    "6. **Seleção dos Dados das Estações Portuguesas**:\r\n",
    "   - Os dados das cinco estações meteorológicas portuguesas selecionadas são extraídos do DataFrame original.\r\n",
    "\r\n",
    "7. **Seleção das Colunas Desejadas**:\r\n",
    "   - São selecionadas as colunas 'id', 'year', 'month', 'element' e os valores diários ('value1' a 'value31').\r\n",
    "\r\n",
    "8. **Exibição dos Dados Selecionados**:\r\n",
    "   - Os dados selecionados são\n",
    "  \n",
    "PS: Neste bloco de código o output diz ser um \"EmptyDataFrama\" devido ao facto de eu ter escolhido o chunk200 e ter trabalhado sempre apenas com ele, e nesse bocado do dataset não existem estações portuguesa. impressos para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4550be5b-595d-4a2f-b1da-6cccd4bc9b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, year, month, element, value1, value2, value3, value4, value5, value6, value7, value8, value9, value10, value11, value12, value13, value14, value15, value16, value17, value18, value19, value20, value21, value22, value23, value24, value25, value26, value27, value28, value29, value30, value31]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ler o arquivo ghcnd-stations.txt para obter informações sobre as estações\n",
    "stations_info = pd.read_fwf(\n",
    "    'global_climate_data/ghcnd-stations.txt',\n",
    "    widths=[11, 9, 10, 7, 31, 3, 3, 3, 5],\n",
    "    header=None\n",
    ")\n",
    "\n",
    "# Renomear as colunas para melhor identificação\n",
    "stations_info.columns = ['id', 'lat', 'long', 'elev', 'name', 'state', 'GSN_FLAG', 'HCN/CRN_FLAG', 'WMO_ID']\n",
    "\n",
    "# Filtrar as estações meteorológicas portuguesas\n",
    "portuguese_stations = stations_info[stations_info['id'].str[:2] == 'PO']\n",
    "\n",
    "# Selecionar as cinco estações desejadas\n",
    "portuguese_station_names = ['Horta', 'Funchal', 'Lisboa', 'Castelo Branco', 'Faro']\n",
    "selected_stations = portuguese_stations[portuguese_stations['name'].isin(portuguese_station_names)]\n",
    "\n",
    "# Ler o arquivo ghcnd_daily.csv\n",
    "chunk200 = pd.read_csv('global_climate_data/ghcnd_daily/chunks/chunks200.csv')\n",
    "\n",
    "# Selecionar os dados das cinco estações meteorológicas portuguesas\n",
    "selected_data = chunk200[chunk200['id'].isin(selected_stations['id'])]\n",
    "\n",
    "# Selecionar as colunas desejadas\n",
    "selected_columns = ['id', 'year', 'month', 'element'] + [f'value{i}' for i in range(1, 32)]\n",
    "selected_data = selected_data[selected_columns]\n",
    "\n",
    "# Mostrar os dados selecionados\n",
    "print(selected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eabf518-af33-46b4-9768-ad2db076d1b5",
   "metadata": {},
   "source": [
    "# Exercicío 7 - Substituir os ids presentes no dataframe pelo correspondente nome da estação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60778291-0710-4926-82d3-5d192d98c910",
   "metadata": {},
   "source": [
    "1. **Leitura do Arquivo de Informações das Estações**:\r\n",
    "   - O arquivo 'ghcnd-stations.txt' é lido usando o método `pd.read_fwf()` para carregar os dados em um DataFrame. As larguras das colunas são especificadas no parâmetro 'widths'.\r\n",
    "   - As colunas do DataFrame resultante são renomeadas para uma melhor identificação dos dados.\r\n",
    "\r\n",
    "2. **Leitura do Arquivo de Dados Diários**:\r\n",
    "   - O arquivo 'chunks200.csv' é lido para obter os dados meteorológicos diários.\r\n",
    "\r\n",
    "3. **Merge entre as Informações da Estação e os Dados Meteorológicos**:\r\n",
    "   - Os dados do arquivo 'chunks200.csv' são mesclados com as informações das estações com base no ID da estação.\r\n",
    "   - A mesclagem é feita usando o método `pd.merge()`.\r\n",
    "\r\n",
    "4. **Substituição do ID pelo Nome da Estação**:\r\n",
    "   - A coluna 'id' é removida do DataFrame resultante.\r\n",
    "   - A coluna 'name', que contém o nome da estação, é renomeada para 'station_name'.\r\n",
    "\r\n",
    "5. **Exibição dos Dados com os Nomes das Estações**:\r\n",
    "   - Os dados resultantes, agora com os IDs substituídos pelo nome da estação, são impresss para análise.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9c2abff-ce7a-4de6-8cc9-495ccebe8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       year  month element  value1  mflag1 qflag1 sflag1  value2  mflag2  \\\n",
      "0      1938      5    TMAX     294     NaN    NaN      0     267     NaN   \n",
      "1      1938      6    TMAX     267     NaN    NaN      0     300     NaN   \n",
      "2      1938      7    TMAX     339     NaN    NaN      0     350     NaN   \n",
      "3      1938      8    TMAX     339     NaN    NaN      0     356     NaN   \n",
      "4      1938      9    TMAX     367     NaN    NaN      0     322     NaN   \n",
      "...     ...    ...     ...     ...     ...    ...    ...     ...     ...   \n",
      "49995  1964      3    TMAX     189     NaN    NaN      0     183     NaN   \n",
      "49996  1964      4    TMAX     150     NaN    NaN      0     239     NaN   \n",
      "49997  1964      5    TMAX     256     NaN    NaN      0     272     NaN   \n",
      "49998  1964      6    TMAX     250     NaN    NaN      0     228     NaN   \n",
      "49999  1964      7    TMAX     300     NaN    NaN      0     333     NaN   \n",
      "\n",
      "      qflag2  ... sflag29  value30  mflag30 qflag30 sflag30  value31  mflag31  \\\n",
      "0        NaN  ...       0      300      NaN     NaN       0      294      NaN   \n",
      "1        NaN  ...       0      311      NaN     NaN       0    -9999      NaN   \n",
      "2        NaN  ...       0      328      NaN     NaN       0      333      NaN   \n",
      "3        NaN  ...       0      294      NaN     NaN       0      378      NaN   \n",
      "4        NaN  ...       0      350      NaN     NaN       0    -9999      NaN   \n",
      "...      ...  ...     ...      ...      ...     ...     ...      ...      ...   \n",
      "49995    NaN  ...     0.0       56      NaN     NaN       0      144      NaN   \n",
      "49996    NaN  ...     0.0      211      NaN     NaN       0    -9999      NaN   \n",
      "49997    NaN  ...     0.0      211      NaN     NaN       0      244      NaN   \n",
      "49998    NaN  ...     0.0      311      NaN     NaN       0    -9999      NaN   \n",
      "49999    NaN  ...     0.0      322      NaN     NaN       0      350      NaN   \n",
      "\n",
      "      qflag31 sflag31         station_name  \n",
      "0         NaN       0  9 MO MT VERNON 3 SW  \n",
      "1         NaN     NaN  9 MO MT VERNON 3 SW  \n",
      "2         NaN       0  9 MO MT VERNON 3 SW  \n",
      "3         NaN       0  9 MO MT VERNON 3 SW  \n",
      "4         NaN     NaN  9 MO MT VERNON 3 SW  \n",
      "...       ...     ...                  ...  \n",
      "49995     NaN       0  0 MO WEBSTER GROVES  \n",
      "49996     NaN     NaN  0 MO WEBSTER GROVES  \n",
      "49997     NaN       0  0 MO WEBSTER GROVES  \n",
      "49998     NaN     NaN  0 MO WEBSTER GROVES  \n",
      "49999     NaN       0  0 MO WEBSTER GROVES  \n",
      "\n",
      "[50000 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ler o arquivo ghcnd-stations.txt para obter informações sobre as estações\n",
    "stations_info = pd.read_fwf('global_climate_data/ghcnd-stations.txt', \n",
    "                            widths=[11, 8, 9, 6, 2, 30, 3, 3, 5],\n",
    "                            header=None,\n",
    "                            names=['id', 'latitude', 'longitude', 'elevation', 'state', 'name', 'GSN_FLAG', 'HCN/CRN_FLAG', 'WMO_ID'])\n",
    "\n",
    "# Ler o arquivo ghcnd_daily.csv\n",
    "chunk200 = pd.read_csv('global_climate_data/ghcnd_daily/chunks/chunks200.csv')\n",
    "\n",
    "# Merge entre as informações da estação e os dados meteorológicos\n",
    "merged_data = pd.merge(chunk200, stations_info[['id', 'name']], on='id', how='left')\n",
    "\n",
    "# Substituir a coluna 'id' pelo nome correspondente da estação\n",
    "merged_data.drop(columns=['id'], inplace=True)\n",
    "merged_data.rename(columns={'name': 'station_name'}, inplace=True)\n",
    "\n",
    "# Mostrar os dados com os IDs substituídos pelo nome da estação\n",
    "print(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216d104-cb74-43cc-a451-53859220228c",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5ed52-446f-4e7e-9765-0f5b757113cb",
   "metadata": {},
   "source": [
    "Ao longo deste trabalho, foi possível consolidar e aplicar diversos conhecimentos relacionados ao processamento de grandes conjuntos de dados, especialmente utilizando a técnica de chunking para lidar com arquivos de tamanho considerável. Através da análise e manipulação desses dados meteorológicos, foram aplicadas técnicas como leitura de arquivos CSV e TXT, tratamento de dados ausentes, conversão de tipos de dados, fusão de DataFrames, seleção e filtragem de dados, entre outras.\n",
    "\n",
    "Além disso, foi possível praticar habilidades relacionadas ao trabalho com DataFrames no Pandas, como renomear colunas, calcular estatísticas descritivas, agrupar dados e aplicar funções de agregação.\n",
    "\n",
    "É importante destacar que, ao longo do processo, recorri ao ChatGPT para esclarecer dúvidas sobre determinadas partes do código, corrigir pequenos erros e ajudar-me a documentar de maneira completa e com maior organização o meu trabalho"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
